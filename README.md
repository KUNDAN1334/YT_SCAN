# YouTube AI Assistant Chrome Extension

A Chrome extension that uses AI to answer questions about YouTube video content using RAG (Retrieval Augmented Generation) with Groq's Llama AI.

## Features

- ğŸ¤– AI-powered Q&A about YouTube videos
- ğŸ“º Automatic transcript extraction
- ğŸ’¬ Chat-like interface
- ğŸ¯ Context-aware responses using RAG
- âš¡ Lightning-fast responses with Groq API
- ğŸ”’ Secure API key management

## Tech Stack

- **Frontend**: HTML, CSS, JavaScript (Chrome Extension)
- **Backend**: FastAPI (Python)
- **AI Model**: Llama 3.1 8B Instant (via Groq API)
- **Libraries**:
  - youtube-transcript-api
  - groq
  - tiktoken
  - python-dotenv
  - uvicorn
  - fastapi

## Installation

### Prerequisites

1. Get a free Groq API key from [console.groq.com](https://console.groq.com)
2. Python 3.8+ installed

### Backend Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/KUNDAN1334/YT_SCAN.git
   cd YT_SCAN
   ```

2. Install Python dependencies:
   ```bash
   pip install fastapi uvicorn youtube-transcript-api groq tiktoken python-dotenv
   ```

3. Create a `.env` file in the project root:
   ```env
   GROQ_API_KEY=your-groq-api-key-here
   ```

4. Start the backend server:
   ```bash
   cd backend
   uvicorn main:app --reload
   ```

### Chrome Extension Setup

1. Open Chrome and go to `chrome://extensions/`
2. Enable "Developer mode"
3. Click "Load unpacked"
4. Select the project root directory (YT_SCAN)

## Usage

1. Make sure your `.env` file contains your Groq API key
2. Start the FastAPI backend server
3. Open the Chrome extension
4. Paste a YouTube URL
5. Click "Load Video"
6. Ask questions about the video content

## Project Structure

```
YT_SCAN/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI server
â”‚   â”œâ”€â”€ transcript_handler.py # YouTube transcript extraction
â”‚   â””â”€â”€ llama_chat.py        # Groq API integration
â”œâ”€â”€ popup.html               # Extension popup UI
â”œâ”€â”€ popup.js                 # Frontend JavaScript
â”œâ”€â”€ style.css               # Styling
â”œâ”€â”€ manifest.json           # Extension manifest
â”œâ”€â”€ .env                    # Environment variables (API keys)
â”œâ”€â”€ .gitignore              # Git ignore file
â””â”€â”€ README.md
```

## Configuration

The AI model is configured in `backend/llama_chat.py` using Groq API:

```python
from groq import Groq
import os
from dotenv import load_dotenv

load_dotenv()
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

def generate_answer(context, question):
    response = client.chat.completions.create(
        model="llama-3.1-8b-instant",  # Fast Llama model
        messages=[{"role": "user", "content": prompt}],
        max_tokens=800,
        temperature=0.1,
        stream=False
    )
    return response.choices[0].message.content.strip()
```

## API Endpoints

- `POST /ask` - Submit question about YouTube video
- `GET /` - Health check

## Model Performance

Groq's Llama 3.1 8B Instant offers:
- âš¡ Ultra-fast response times (1-3 seconds)
- ğŸ¯ High-quality text generation
- ğŸ§  Excellent context understanding
- ğŸ’° Free tier with generous limits
- ğŸ”„ No local setup required

## Token Management

The extension automatically handles large video transcripts by:
- ğŸ“Š Counting tokens using tiktoken
- âœ‚ï¸ Smart context truncation (keeps recent content)
- ğŸ›¡ï¸ Preventing API limit errors
- ğŸ¯ Maintaining answer quality

## Environment Variables

Create a `.env` file with:
```env
GROQ_API_KEY=your-actual-groq-api-key-here
```

**Important**: Never commit your `.env` file to version control!

## Troubleshooting

1. **API Key Error**: 
   - Check your `.env` file exists and contains valid Groq API key
   - Verify API key at [console.groq.com](https://console.groq.com)

2. **Token Limit Error**: 
   - The extension automatically truncates long transcripts
   - If issues persist, try shorter video segments

3. **Connection Error**: 
   - Ensure FastAPI server is running on `http://localhost:8000`
   - Check your internet connection for Groq API access

4. **Transcript Unavailable**: 
   - Some videos may not have transcripts available
   - Try videos with auto-generated captions

## Rate Limits

**Groq Free Tier:**
- 6,000 tokens per minute
- Automatic context truncation handles this
- Upgrade to Dev Tier for higher limits

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## Security

- âœ… API keys stored in `.env` file
- âœ… `.env` file excluded from git
- âœ… No sensitive data in code
- âœ… Secure API communication

## License

MIT License

## Changelog

### v2.0.0
- âš¡ Switched from local Mistral to Groq API
- ğŸš€ 10x faster response times
- ğŸ”§ Added automatic token management
- ğŸ”’ Improved security with environment variables
- ğŸ“Š Added response time monitoring

### v1.0.0
- ğŸ‰ Initial release with local Mistral model

## Acknowledgments

- [Groq](https://groq.com/) for lightning-fast AI inference
- [Meta](https://ai.meta.com/) for the Llama model
- [YouTube Transcript API](https://github.com/jdepoix/youtube-transcript-api) for transcript extraction
- [FastAPI](https://fastapi.tiangolo.com/) for the backend framework
```

Now commit these changes:

```bash
git add .
```

```bash
git commit -m "Update README: Switch from Mistral to Groq API with Llama 3.1"
```

```bash
git push
